#!/usr/bin/python3

# This script transforms the csv data generated by bloaty into data that is
# ready to upload to BigQuery. It adds columns for the toolchain version, target
# platform and runtime environment, and performs several other transformations.

import argparse
import os
import pandas as pd
from datetime import datetime
from bigquery_schema import BQ_SCHEMA

def strip_prefix_from_column_values(csv_data, column_name, prefix):
    csv_data[column_name] = csv_data[column_name].apply(lambda x: x[len(prefix):])

def add_column(csv_data, column_name, column_value):
    allow_duplicate_column_labels = False
    csv_data.insert(0, column_name, column_value, allow_duplicate_column_labels)

def rename_column(csv_data, old_name, new_name):
    csv_data.rename(columns={old_name: new_name}, inplace=True)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('bloaty_csv_output', type=str, help='The path to the csv file generated by bloaty')
    parser.add_argument('output_csv', type=str, help='Where to write the generated csv file')

    req_group = parser.add_argument_group('required flags', 'Additional properties that help slice and group data in BigQuery')
    req_group.add_argument('--toolchain-version', type=str, help='The toolchain version that corresponds to the input data')

    opt_group = parser.add_argument_group('optional flags')
    opt_group.add_argument('--creation-time', type=str, help='timestamp when the release was created formatted as YYYY-MM-DD')
    opt_group.add_argument('--environment', type=str, default='debug', help='The name of the environment where this data was generated')
    opt_group.add_argument('--strip-inputfiles-prefix', type=str, help='A prefix to strip from the inputfiles column')
    args = parser.parse_args()

    csv_data = pd.read_csv(args.bloaty_csv_output)

    creation_time = args.creation_time
    if len(creation_time) == 0:
        creation_time = datetime.now().replace(microsecond=0).isoformat()

    if args.strip_inputfiles_prefix:
        strip_inputfiles_prefix = os.path.abspath(args.strip_inputfiles_prefix) + os.sep
        strip_prefix_from_column_values(csv_data, 'inputfiles', strip_inputfiles_prefix)
    add_column(csv_data, 'toolchain_version', args.toolchain_version)
    add_column(csv_data, 'environment', args.environment)
    add_column(csv_data, 'target_os', 'windows')
    add_column(csv_data, 'target_arch', 'amd64')
    add_column(csv_data, 'creation_time', creation_time)
    rename_column(csv_data, 'inputfiles', 'filename')
    rename_column(csv_data, 'segments', 'segment')

    # Reorder the columns to match the order specified in BQ_SCHEMA.
    # Otherwise bigquery fails to parse the columns.
    column_order = [ schema_field.name for schema_field in BQ_SCHEMA ]
    csv_data = csv_data[column_order]

    csv_data.to_csv(args.output_csv, index=False)

main()